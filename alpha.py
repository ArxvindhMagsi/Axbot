# -*- coding: utf-8 -*-
"""alpha.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1koZjqwotsIGl9oKRjXo_PPE3tbCBleSw
"""

#!/usr/bin/env python3
"""
Advanced Medical AI Agent with Multi-Source Intelligence
Complete backend implementation for Google Colab
"""

import os
import asyncio
import json
import logging
import requests
from typing import List, Dict, Any, Optional, Union
from dataclasses import dataclass
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from PIL import Image
import io
import base64
from urllib.parse import quote
import warnings
warnings.filterwarnings('ignore')

# Install required packages
def install_requirements():
    """Install all required packages for the medical AI agent"""
    packages = [
        "langchain",
        "langchain-community",
        "langchain-groq",
        "arxiv",
        "wikipedia-api",
        "tavily-python",
        "google-search-results",
        "requests",
        "beautifulsoup4",
        "matplotlib",
        "seaborn",
        "pandas",
        "Pillow",
        "python-dotenv",
        "openai",
        "groq",
        "wikipedia" # Added wikipedia here
    ]

    import subprocess
    import sys

    for package in packages:
        try:
            subprocess.check_call([sys.executable, "-m", "pip", "install", package])
            print(f"✅ Installed {package}")
        except Exception as e:
            print(f"❌ Failed to install {package}: {e}")

def import_libraries():
    """Import libraries after installation"""
    try:
        from langchain.agents import AgentExecutor, create_openai_tools_agent
        from langchain_community.tools import ArxivQueryRun, WikipediaQueryRun
        from langchain_community.utilities import ArxivAPIWrapper, WikipediaAPIWrapper
        from langchain_groq import ChatGroq
        from langchain.tools import Tool
        from langchain.schema import SystemMessage, HumanMessage
        from tavily import TavilyClient
        from serpapi import GoogleSearch
        # ✅ Add these imports at the top of your file
        from langchain_community.utilities import ArxivAPIWrapper
        from langchain_community.tools import ArxivQueryRun
        import groq

        return (AgentExecutor, create_openai_tools_agent, ArxivQueryRun, WikipediaQueryRun,
                ArxivAPIWrapper, WikipediaAPIWrapper, ChatGroq, Tool, SystemMessage,
                HumanMessage, TavilyClient, GoogleSearch, groq)
    except ImportError as e:
        print(f"Import error: {e}")
        print("Please run install_requirements() first")
        return None


# Uncomment this line when running in Colab for the first time
install_requirements()

# Import after installation
imported_libs = import_libraries()

if imported_libs:
    (AgentExecutor, create_openai_tools_agent, ArxivQueryRun, WikipediaQueryRun,
     ArxivAPIWrapper, WikipediaAPIWrapper, ChatGroq, Tool, SystemMessage,
     HumanMessage, TavilyClient, GoogleSearch, groq) = imported_libs
else:
    # Handle the case where imports failed (e.g., exit or raise an error)
    raise ImportError("Required libraries could not be imported. Please check installation.")


# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class SearchResult:
    """Data class for search results"""
    source: str
    title: str
    content: str
    url: str
    relevance_score: float
    metadata: Dict[str, Any]
    image_url: Optional[str] = None

@dataclass
class AgentResponse:
    """Data class for agent responses"""
    answer: str
    sources: List[SearchResult]
    confidence: float
    processing_time: float
    visualizations: List[Dict[str, Any]]
    reasoning: str

class MedicalSearchTools:
    """Advanced medical search tools integration"""

    def __init__(self, api_keys: Dict[str, str]):
        self.api_keys = api_keys
        self.setup_tools()

    def setup_tools(self):
        """Initialize all search tools"""
        # ArXiv for research papers
        self.arxiv_wrapper = ArxivAPIWrapper(top_k_results=5, doc_content_chars_max=2000)
        self.arxiv_tool = ArxivQueryRun(api_wrapper=self.arxiv_wrapper)

        # Wikipedia for general medical knowledge
        self.wiki_wrapper = WikipediaAPIWrapper(top_k_results=3, doc_content_chars_max=2000)
        self.wiki_tool = WikipediaQueryRun(api_wrapper=self.wiki_wrapper)

        # Tavily for comprehensive web search
        if self.api_keys.get('tavily'):
            self.tavily_client = TavilyClient(api_key=self.api_keys['tavily'])

        # SerpAPI for Google search
        if self.api_keys.get('serper'):
            self.serper_key = self.api_keys['serper']

    async def search_arxiv(self, query: str) -> List[SearchResult]:
        """Search ArXiv for medical research papers"""
        try:
            results = self.arxiv_tool.run(query)
            search_results = []

            # Parse ArXiv results
            papers = results.split('\n\n')
            for paper in papers[:3]:
                if 'Title:' in paper and 'Summary:' in paper:
                    lines = paper.split('\n')
                    title = next((line.replace('Title: ', '') for line in lines if line.startswith('Title:')), '')
                    summary = next((line.replace('Summary: ', '') for line in lines if line.startswith('Summary:')), '')

                    search_results.append(SearchResult(
                        source='ArXiv',
                        title=title,
                        content=summary,
                        url=f"https://arxiv.org/search/?query={quote(query)}",
                        relevance_score=0.8,
                        metadata={'type': 'research_paper', 'domain': 'medical_research'},
                        image_url='https://arxiv.org/icons/cu/cornell-reduced-white-SMALL.svg'
                    ))

            return search_results

        except Exception as e:
            logger.error(f"ArXiv search error: {e}")
            return []

    async def search_wikipedia(self, query: str) -> List[SearchResult]:
        """Search Wikipedia for medical information"""
        try:
            results = self.wiki_tool.run(query)
            search_results = []

            # Parse Wikipedia results
            sections = results.split('Page: ')
            for section in sections[1:3]:  # Skip first empty section
                lines = section.split('\n')
                title = lines[0] if lines else ''
                content = '\n'.join(lines[1:5]) if len(lines) > 1 else ''

                search_results.append(SearchResult(
                    source='Wikipedia',
                    title=title,
                    content=content[:500] + '...' if len(content) > 500 else content,
                    url=f"https://en.wikipedia.org/wiki/{quote(title.replace(' ', '_'))}",
                    relevance_score=0.7,
                    metadata={'type': 'encyclopedia', 'domain': 'general_medical'},
                    image_url='https://upload.wikimedia.org/wikipedia/commons/thumb/8/80/Wikipedia-logo-v2.svg/200px-Wikipedia-logo-v2.svg.png'
                ))

            return search_results

        except Exception as e:
            logger.error(f"Wikipedia search error: {e}")
            return []

    async def search_tavily(self, query: str) -> List[SearchResult]:
        """Search web using Tavily for comprehensive results"""
        try:
            if not hasattr(self, 'tavily_client'):
                return []

            response = self.tavily_client.search(
                query=f"medical {query}",
                search_depth="advanced",
                max_results=5
            )

            search_results = []
            for result in response.get('results', []):
                search_results.append(SearchResult(
                    source='Tavily',
                    title=result.get('title', ''),
                    content=result.get('content', '')[:500],
                    url=result.get('url', ''),
                    relevance_score=result.get('score', 0.5),
                    metadata={'type': 'web_search', 'domain': 'medical_web'},
                    image_url=None
                ))

            return search_results

        except Exception as e:
            logger.error(f"Tavily search error: {e}")
            return []

    async def search_serper(self, query: str) -> List[SearchResult]:
        """Search using SerpAPI for Google results"""
        try:
            if not hasattr(self, 'serper_key'):
                return []

            params = {
                "engine": "google",
                "q": f"medical {query}",
                "api_key": self.serper_key,
                "num": 5
            }

            search = GoogleSearch(params)
            results = search.get_dict()

            search_results = []
            for result in results.get('organic_results', []):
                search_results.append(SearchResult(
                    source='Google',
                    title=result.get('title', ''),
                    content=result.get('snippet', ''),
                    url=result.get('link', ''),
                    relevance_score=0.6,
                    metadata={'type': 'search_engine', 'domain': 'medical_web'},
                    image_url=result.get('thumbnail', None)
                ))

            return search_results

        except Exception as e:
            logger.error(f"SerpAPI search error: {e}")
            return []

class MedicalVisualizationEngine:
    """Advanced visualization engine for medical data"""

    @staticmethod
    def create_source_distribution_chart(sources: List[SearchResult]) -> str:
        """Create a chart showing distribution of sources"""
        source_counts = {}
        for result in sources:
            source_counts[result.source] = source_counts.get(result.source, 0) + 1

        plt.figure(figsize=(10, 6))
        colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']

        plt.pie(source_counts.values(), labels=source_counts.keys(),
                colors=colors, autopct='%1.1f%%', startangle=90)
        plt.title('Medical Information Sources Distribution', fontsize=16, fontweight='bold')

        # Save to base64
        buffer = io.BytesIO()
        plt.savefig(buffer, format='png', dpi=300, bbox_inches='tight')
        buffer.seek(0)
        image_base64 = base64.b64encode(buffer.getvalue()).decode()
        plt.close()

        return image_base64

    @staticmethod
    def create_relevance_score_chart(sources: List[SearchResult]) -> str:
        """Create a chart showing relevance scores by source"""
        df = pd.DataFrame([
            {'Source': result.source, 'Relevance': result.relevance_score}
            for result in sources
        ])

        plt.figure(figsize=(12, 6))
        sns.barplot(data=df, x='Source', y='Relevance', palette='viridis')
        plt.title('Source Relevance Scores', fontsize=16, fontweight='bold')
        plt.ylabel('Relevance Score', fontsize=12)
        plt.xlabel('Information Source', fontsize=12)
        plt.xticks(rotation=45)

        # Save to base64
        buffer = io.BytesIO()
        plt.savefig(buffer, format='png', dpi=300, bbox_inches='tight')
        buffer.seek(0)
        image_base64 = base64.b64encode(buffer.getvalue()).decode()
        plt.close()

        return image_base64

    @staticmethod
    def create_medical_comparison_chart(data: Dict[str, Any]) -> str:
        """Create comparison charts for medical data"""
        plt.figure(figsize=(14, 8))

        # Example: Drug comparison or symptom analysis
        if 'comparison' in data:
            categories = list(data['comparison'].keys())
            values = list(data['comparison'].values())

            plt.barh(categories, values, color='skyblue')
            plt.title('Medical Data Comparison', fontsize=16, fontweight='bold')
            plt.xlabel('Value/Score', fontsize=12)
        else:
            # Default visualization
            plt.text(0.5, 0.5, 'Medical Analysis Visualization\nData processing complete',
                    ha='center', va='center', fontsize=14, transform=plt.gca().transAxes)
            plt.title('Medical AI Analysis', fontsize=16, fontweight='bold')

        # Save to base64
        buffer = io.BytesIO()
        plt.savefig(buffer, format='png', dpi=300, bbox_inches='tight')
        buffer.seek(0)
        image_base64 = base64.b64encode(buffer.getvalue()).decode()
        plt.close()

        return image_base64

class MedicalAIAgent:
    """Advanced Medical AI Agent with multi-source intelligence"""

    def __init__(self, api_keys: Dict[str, str]):
        self.api_keys = api_keys
        self.search_tools = MedicalSearchTools(api_keys)
        self.viz_engine = MedicalVisualizationEngine()

        # Initialize Groq LLM
        if api_keys.get('groq'):
            self.llm = ChatGroq(
                groq_api_key=api_keys['groq'],
                model_name="llama-3.1-8b-instant", # Updated model name
                temperature=0.3,
                max_tokens=2000
            )
        else:
            logger.warning("No Groq API key provided")
            self.llm = None

    async def intelligent_search(self, query: str, tools: List[str] = None) -> List[SearchResult]:
        """Perform intelligent multi-source search"""
        if tools is None:
            tools = ['arxiv', 'wikipedia', 'tavily', 'serper']

        all_results = []

        # Execute searches concurrently
        search_tasks = []

        if 'arxiv' in tools:
            search_tasks.append(self.search_tools.search_arxiv(query))
        if 'wikipedia' in tools:
            search_tasks.append(self.search_tools.search_wikipedia(query))
        if 'tavily' in tools:
            search_tasks.append(self.search_tools.search_tavily(query))
        search_tasks.append(self.search_tools.search_serper(query))

        # Wait for all searches to complete
        search_results = await asyncio.gather(*search_tasks, return_exceptions=True)

        # Combine results
        for results in search_results:
            if isinstance(results, list):
                all_results.extend(results)

        # Sort by relevance score
        all_results.sort(key=lambda x: x.relevance_score, reverse=True)

        return all_results[:10]  # Return top 10 results

    def analyze_with_llm(self, query: str, search_results: List[SearchResult]) -> Dict[str, Any]:
        """Analyze search results using LLM"""
        if not self.llm:
            return {
                'analysis': 'LLM analysis not available - please provide Groq API key',
                'confidence': 0.0,
                'reasoning': 'No LLM configured'
            }

        # Prepare context from search results
        context = "\n\n".join([
            f"Source: {result.source}\nTitle: {result.title}\nContent: {result.content}\nURL: {result.url}"
            for result in search_results[:5]  # Use top 5 results
        ])

        # Create system message for medical AI
        system_prompt = """You are an advanced Medical AI Assistant with access to multiple authoritative sources.

        Your capabilities:
        1. Analyze medical information from research papers, encyclopedias, and web sources
        2. Provide evidence-based medical insights
        3. Compare and synthesize information from multiple sources
        4. Identify potential drug interactions, side effects, and contraindications
        5. Suggest relevant medical tests or consultations when appropriate

        IMPORTANT DISCLAIMERS:
        - Always recommend consulting healthcare professionals for medical decisions
        - Provide educational information, not personalized medical advice
        - Highlight when information is preliminary or requires further validation

        Response format:
        1. Direct answer to the query
        2. Evidence from sources (cite specific sources)
        3. Confidence level (0.0-1.0)
        4. Reasoning for the confidence level
        5. Recommendations for further action if applicable
        """

        user_prompt = f"""
        Medical Query: {query}

        Available Information Sources:
        {context}

        Please provide a comprehensive medical analysis based on the available sources.
        Focus on accuracy, evidence-based information, and appropriate medical disclaimers.
        """

        try:
            response = self.llm.invoke([
                SystemMessage(content=system_prompt),
                HumanMessage(content=user_prompt)
            ])

            analysis = response.content

            # Extract confidence (simple heuristic based on source quality and quantity)
            confidence = min(0.9, len(search_results) * 0.15 + 0.3)

            return {
                'analysis': analysis,
                'confidence': confidence,
                'reasoning': f'Analysis based on {len(search_results)} sources with varying authority levels'
            }

        except Exception as e:
            logger.error(f"LLM analysis error: {e}")
            return {
                'analysis': f'Error in LLM analysis: {str(e)}',
                'confidence': 0.0,
                'reasoning': 'LLM processing failed'
            }

    async def process_medical_query(self, query: str, tools: List[str] = None) -> AgentResponse:
        """Process a medical query with full intelligence pipeline"""
        start_time = datetime.now()

        logger.info(f"Processing medical query: {query}")

        # Step 1: Intelligent multi-source search
        search_results = await self.intelligent_search(query, tools)
        logger.info(f"Found {len(search_results)} search results")

        # Step 2: LLM analysis and synthesis
        llm_analysis = self.analyze_with_llm(query, search_results)

        # Step 3: Generate visualizations
        visualizations = []

        if search_results:
            # Source distribution chart
            source_chart = self.viz_engine.create_source_distribution_chart(search_results)
            visualizations.append({
                'type': 'source_distribution',
                'title': 'Information Sources Distribution',
                'image_base64': source_chart
            })

            # Relevance scores chart
            relevance_chart = self.viz_engine.create_relevance_score_chart(search_results)
            visualizations.append({
                'type': 'relevance_scores',
                'title': 'Source Relevance Analysis',
                'image_base64': relevance_chart
            })

            # Medical comparison chart (if applicable)
            comparison_chart = self.viz_engine.create_medical_comparison_chart({})
            visualizations.append({
                'type': 'medical_analysis',
                'title': 'Medical Data Analysis',
                'image_base64': comparison_chart
            })

        # Step 4: Calculate processing time
        processing_time = (datetime.now() - start_time).total_seconds()

        # Step 5: Create comprehensive response
        response = AgentResponse(
            answer=llm_analysis['analysis'],
            sources=search_results,
            confidence=llm_analysis['confidence'],
            processing_time=processing_time,
            visualizations=visualizations,
            reasoning=llm_analysis['reasoning']
        )

        logger.info(f"Query processed in {processing_time:.2f} seconds")
        return response

class MedicalChatInterface:
    """Interactive chat interface for the medical AI agent"""

    def __init__(self, api_keys: Dict[str, str]):
        self.agent = MedicalAIAgent(api_keys)
        self.conversation_history = []

    def display_response(self, response: AgentResponse):
        """Display formatted response"""
        print("\n" + "="*80)
        print("🏥 MEDICAL AI ASSISTANT RESPONSE")
        print("="*80)

        print(f"\n📝 ANALYSIS:")
        print("-" * 40)
        print(response.answer)

        print(f"\n📊 CONFIDENCE LEVEL: {response.confidence:.1%}")
        print(f"⚡ PROCESSING TIME: {response.processing_time:.2f} seconds")

        print(f"\n🔍 INFORMATION SOURCES ({len(response.sources)}):")
        print("-" * 40)
        for i, source in enumerate(response.sources, 1):
            print(f"{i}. [{source.source}] {source.title}")
            print(f"   📎 {source.url}")
            print(f"   ⭐ Relevance: {source.relevance_score:.2f}")
            if source.content:
                preview = source.content[:100] + "..." if len(source.content) > 100 else source.content
                print(f"   💬 {preview}")
            print()

        print(f"\n📈 VISUALIZATIONS ({len(response.visualizations)}):")
        print("-" * 40)
        for viz in response.visualizations:
            print(f"📊 {viz['title']} ({viz['type']})")
            print("   [Visualization generated - view in notebook output]")

        print(f"\n🧠 REASONING:")
        print("-" * 40)
        print(response.reasoning)

        print("\n⚠️  MEDICAL DISCLAIMER:")
        print("-" * 40)
        print("This information is for educational purposes only.")
        print("Always consult with healthcare professionals for medical advice.")

        print("="*80)

    async def chat(self):
        """Start interactive chat session"""
        print("\n🏥 Welcome to Medical AI Assistant!")
        print("Type 'quit' to exit, 'help' for commands")
        print("="*50)

        while True:
            try:
                query = input("\n💬 Your medical question: ").strip()

                if query.lower() == 'quit':
                    print("👋 Goodbye! Stay healthy!")
                    break

                if query.lower() == 'help':
                    print("\nAvailable commands:")
                    print("- Ask any medical question")
                    print("- Type 'quit' to exit")
                    print("- Type 'history' to see conversation history")
                    continue

                if query.lower() == 'history':
                    print(f"\n📚 Conversation History ({len(self.conversation_history)} items):")
                    for i, item in enumerate(self.conversation_history, 1):
                        print(f"{i}. Q: {item['query'][:50]}...")
                        print(f"   A: {item['response'][:50]}...")
                    continue

                if not query:
                    continue

                print("\n🔄 Processing your medical query...")
                print("🔍 Searching multiple medical databases...")

                # Process the query
                response = await self.agent.process_medical_query(query)

                # Display the response
                self.display_response(response)

                # Save to history
                self.conversation_history.append({
                    'query': query,
                    'response': response.answer,
                    'timestamp': datetime.now().isoformat(),
                    'confidence': response.confidence
                })

                # Display visualizations
                for viz in response.visualizations:
                    img_data = base64.b64decode(viz['image_base64'])
                    img = Image.open(io.BytesIO(img_data))
                    plt.figure(figsize=(10, 6))
                    plt.imshow(img)
                    plt.axis('off')
                    plt.title(viz['title'])
                    plt.show()

            except KeyboardInterrupt:
                print("\n\n👋 Session interrupted. Goodbye!")
                break
            except Exception as e:
                print(f"\n❌ Error: {e}")
                logger.error(f"Chat error: {e}")

# Example usage and setup
def setup_medical_ai():
    """Setup the medical AI system"""
    print("🚀 Setting up Medical AI Assistant...")

    # API Keys - Replace with your actual keys
    api_keys = {
        'groq': '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',  # Your Groq API key
        'tavily': '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',  # Your Tavily API key
        'serper': 'e9cda3b09e798b6dd0576800013125e3a2762a8f',  # Your SerpAPI key
        'openrouter': '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'  # Your OpenRouter API key (if using)
    }

    print("⚠️  Please add your API keys to the api_keys dictionary")
    print("Available API keys to configure:")
    for key, value in api_keys.items():
        status = "✅ Configured" if value else "❌ Missing"
        print(f"  - {key}: {status}")

    return MedicalChatInterface(api_keys)

# Main execution
if __name__ == "__main__":
    # For Google Colab execution
    print("🏥 Medical AI Agent - Advanced Intelligence System")
    print("="*60)

    # Setup the system
    chat_interface = setup_medical_ai()

    # Example usage
    print("\n📋 Example Queries:")
    print("- 'What are the side effects of aspirin?'")
    print("- 'Compare effectiveness of different diabetes medications'")
    print("- 'Latest research on COVID-19 treatments'")
    print("- 'Drug interactions between warfarin and antibiotics'")

    # Uncomment to start interactive chat
    # await chat_interface.chat()

await chat_interface.chat()
